ğŸ§  K-Nearest Neighbors (KNN) Algorithm
---

âœ… Definition

> K-Nearest Neighbors (KNN) is a supervised learning algorithm used for classification and regression. It predicts the output for a new data point by looking at the â€˜Kâ€™ closest data points (neighbors) from the training set.
---

ğŸ”§ How KNN Works (Step-by-Step)

1. Choose the number of neighbors, K (e.g., K = 3).


2. Measure the distance between the new data point and all training data.


3. Identify the K nearest neighbors (based on the smallest distances).


4. For classification:

Take a majority vote among the K neighbors.


5. For regression:

Take the average of the K neighborsâ€™ values.


KNN for Classification vs Regression

Task	KNN does this

Classification	Majority voting among neighbors
Regression	Average of neighborsâ€™ target values


---

ğŸ“ˆ Advantages of KNN

âœ… Simple to implement
âœ… Works well with small datasets
âœ… No training phase â€” lazy learner


---

âš  Disadvantages of KNN

âŒ Slow with large datasets (computes distance to all points)
âŒ Sensitive to irrelevant features and scaling
âŒ Struggles with high-dimensional data (curse of dimensionality)